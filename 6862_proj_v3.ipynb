{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning related imports\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, r2_score, classification_report,\n",
    "    roc_auc_score, accuracy_score, precision_score,\n",
    "    recall_score, f1_score\n",
    ")\n",
    "\n",
    "# Import models\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "# Configure warnings and display options\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"10k_diabetes (1).csv\")\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nDataset Info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '?' with 'Unknown' for categorical variables\n",
    "df['weight'] = df['weight'].replace('?', 'Unknown')\n",
    "df['medical_specialty'] = df['medical_specialty'].replace('?', 'Unknown')\n",
    "df['payer_code'] = df['payer_code'].replace(\"?\", 'Unknown')\n",
    "\n",
    "# Fill missing admission types with mode\n",
    "df['admission_type_id'] = df['admission_type_id'].fillna(df['admission_type_id'].mode()[0])\n",
    "df['admission_source_id'] = df['admission_source_id'].fillna(df['admission_source_id'].mode()[0])\n",
    "\n",
    "# Convert age ranges to numeric midpoints\n",
    "def age_to_midpoint(age_range):\n",
    "    if pd.isna(age_range) or age_range == '?':\n",
    "        return None\n",
    "    nums = [int(x) for x in age_range.strip('[]()').split('-')]\n",
    "    return sum(nums) / 2\n",
    "\n",
    "df['age_numeric'] = df['age'].apply(age_to_midpoint)\n",
    "\n",
    "# Verify changes\n",
    "print(\"Unique values in weight column:\", df['weight'].unique())\n",
    "print(\"\\nUnique values in age_numeric:\", df['age_numeric'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary readmission target\n",
    "df['readmitted_30'] = df['readmitted'].astype(int)\n",
    "\n",
    "# Visualize readmission distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='readmitted_30', palette='viridis')\n",
    "plt.title('Distribution of 30-day Readmissions')\n",
    "plt.xlabel('Readmitted (1: Yes, 0: No)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for i in plt.gca().containers:\n",
    "    plt.gca().bar_label(i)\n",
    "\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Print readmission statistics\n",
    "readmission_rate = df['readmitted_30'].mean() * 100\n",
    "print(f\"\\nReadmission Rate: {readmission_rate:.2f}%\")\n",
    "print(\"\\nDetailed Statistics:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Total patients:\", len(df))\n",
    "print(\"Number of readmissions:\", df['readmitted_30'].sum())\n",
    "print(\"Number of non-readmissions:\", len(df) - df['readmitted_30'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_diagnosis(diag_desc):\n",
    "    if pd.isna(diag_desc):\n",
    "        return 'Unknown'\n",
    "    diag_lower = str(diag_desc).lower()\n",
    "    if 'diabetes' in diag_lower:\n",
    "        return 'Diabetes'\n",
    "    elif any(x in diag_lower for x in ['heart', 'cardiac', 'coronary']):\n",
    "        return 'Cardiovascular'\n",
    "    elif any(x in diag_lower for x in ['respiratory', 'pneumonia', 'copd']):\n",
    "        return 'Respiratory'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Categorize and create dummy variables\n",
    "df['primary_diagnosis_category'] = df['diag_1_desc'].apply(categorize_diagnosis)\n",
    "diagnosis_dummies = pd.get_dummies(df['primary_diagnosis_category'], prefix='diag_cat')\n",
    "df = pd.concat([df, diagnosis_dummies], axis=1)\n",
    "\n",
    "# Visualize diagnosis distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='primary_diagnosis_category')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Distribution of Primary Diagnosis Categories')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create health score\n",
    "numerical_cols = ['num_lab_procedures', 'num_procedures', 'num_medications']\n",
    "scaler = StandardScaler()\n",
    "normalized_vals = scaler.fit_transform(df[numerical_cols])\n",
    "df['health_score'] = (normalized_vals[:, 0] * 0.3 + \n",
    "                     normalized_vals[:, 1] * 0.3 + \n",
    "                     normalized_vals[:, 2] * 0.4)\n",
    "\n",
    "# Visualize health score\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=df, x='health_score', hue='readmitted_30', multiple=\"stack\")\n",
    "plt.title('Health Score Distribution by Readmission Status')\n",
    "plt.xlabel('Health Score')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Feature encodings\n",
    "nominal_features = ['race', 'gender', 'medical_specialty']\n",
    "for feature in nominal_features:\n",
    "    dummies = pd.get_dummies(df[feature], prefix=feature)\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    df = df.drop(feature, axis=1)\n",
    "\n",
    "ordinal_features = ['discharge_disposition_id', 'admission_source_id']\n",
    "le = LabelEncoder()\n",
    "for feature in ordinal_features:\n",
    "    df[f'{feature}_encoded'] = le.fit_transform(df[feature].astype(str))\n",
    "\n",
    "# Create interaction features\n",
    "df['procedure_medication_interaction'] = df['num_procedures'] * df['num_medications']  # Changed from total_medications\n",
    "df['lab_medication_interaction'] = df['num_lab_procedures'] * df['num_medications']  # Changed from total_medications\n",
    "\n",
    "# Create complexity scores\n",
    "df['medical_complexity'] = (\n",
    "    df['num_lab_procedures'] * 0.4 +\n",
    "    df['num_procedures'] * 0.3 +\n",
    "    df['num_medications'] * 0.3 +  # Changed from total_medications\n",
    "    (df['diag_cat_Diabetes'].fillna(0) * 0.2) +\n",
    "    (df['diag_cat_Cardiovascular'].fillna(0) * 0.2)\n",
    ")\n",
    "\n",
    "df['diagnosis_severity'] = (\n",
    "    (df['diag_cat_Diabetes'].fillna(0) * 3) +\n",
    "    (df['diag_cat_Cardiovascular'].fillna(0) * 2) +\n",
    "    (df['diag_cat_Respiratory'].fillna(0) * 2) +\n",
    "    (df['diag_cat_Other'].fillna(0) * 1)\n",
    ")\n",
    "\n",
    "# Let's verify the columns we have\n",
    "print(\"\\nAvailable columns in the dataset:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total features selected: 78\n",
      "\n",
      "Feature shape: (10000, 78)\n",
      "Number of resource targets: 3\n",
      "Readmission target shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "def prepare_modeling_features(df):\n",
    "    \"\"\"Prepare feature sets for modeling tasks\"\"\"\n",
    "    \n",
    "    common_features = [\n",
    "        'age_numeric', 'time_in_hospital', 'num_lab_procedures',\n",
    "        'num_procedures', 'num_medications', 'health_score',  # Changed from total_medications\n",
    "        'medical_complexity', 'diagnosis_severity',\n",
    "        'procedure_medication_interaction', 'lab_medication_interaction',\n",
    "        'discharge_disposition_id_encoded', 'admission_source_id_encoded'\n",
    "    ]\n",
    "    \n",
    "    # Get all dummy columns\n",
    "    dummy_cols = [col for col in df.columns if any(x in col for x in \n",
    "                 ['diag_cat_', 'race_', 'gender_', 'medical_specialty_'])]\n",
    "    \n",
    "    # Combine features\n",
    "    feature_cols = common_features + dummy_cols\n",
    "    \n",
    "    # Select only existing columns\n",
    "    existing_cols = [col for col in feature_cols if col in df.columns]\n",
    "    \n",
    "    print(f\"Total features selected: {len(existing_cols)}\")\n",
    "    return existing_cols\n",
    "\n",
    "# Prepare features\n",
    "selected_features = prepare_modeling_features(df)\n",
    "X = df[selected_features]\n",
    "\n",
    "# Prepare targets for both tasks\n",
    "resource_targets = {\n",
    "    'length_of_stay': df['time_in_hospital'],\n",
    "    'num_procedures': df['num_procedures'],\n",
    "    'num_medications': df['num_medications']  # Changed from total_medications\n",
    "}\n",
    "y_readmission = df['readmitted_30']\n",
    "\n",
    "print(\"\\nFeature shape:\", X.shape)\n",
    "print(\"Number of resource targets:\", len(resource_targets))\n",
    "print(\"Readmission target shape:\", y_readmission.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Update resource optimization models\n",
    "resource_models = {\n",
    "    'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge': Ridge(random_state=42),\n",
    "    'Lasso': Lasso(random_state=42),\n",
    "    'SVR': SVR(),\n",
    "    'LightGBM': lgb.LGBMRegressor(random_state=42),\n",
    "    'TensorFlow': 'tensorflow'  # Placeholder for TensorFlow model\n",
    "}\n",
    "\n",
    "# Update readmission models\n",
    "readmission_models = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'SVC': SVC(probability=True, random_state=42),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=42),\n",
    "    'LightGBM': lgb.LGBMClassifier(random_state=42),\n",
    "    'TensorFlow': 'tensorflow'  # Placeholder for TensorFlow model\n",
    "}\n",
    "\n",
    "# Update parameter grids (remove Neural Network parameters as TensorFlow will be handled separately)\n",
    "param_grid_resource = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'max_depth': [None, 10, 20]\n",
    "    },\n",
    "    'Linear Regression': {},\n",
    "    'Ridge': {\n",
    "        'alpha': [0.1, 1.0, 10.0]\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'alpha': [0.1, 1.0, 10.0]\n",
    "    },\n",
    "    'SVR': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'num_leaves': [31, 50],\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'n_estimators': [50, 100]\n",
    "    }\n",
    "}\n",
    "\n",
    "param_grid_readmission = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'class_weight': ['balanced', None]\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'class_weight': ['balanced', None]\n",
    "    },\n",
    "    'SVC': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'class_weight': ['balanced', None]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [50, 100],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'max_depth': [3, 6]\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'num_leaves': [31, 50],\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'n_estimators': [50, 100]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define TensorFlow model architectures\n",
    "def create_tf_regression_model(input_dim):\n",
    "    \"\"\"Create TensorFlow model for resource optimization\"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_dim=input_dim),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dropout(0.1),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dense(1)  # Output layer for regression\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                 loss='mse',\n",
    "                 metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def create_tf_classification_model(input_dim):\n",
    "    \"\"\"Create TensorFlow model for readmission prediction\"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_dim=input_dim),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dropout(0.1),\n",
    "        layers.Dense(16, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                 loss='binary_crossentropy',\n",
    "                 metrics=['accuracy', tf.keras.metrics.AUC()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Training Resource Optimization Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train resource optimization models\n",
    "def train_resource_models(X, y, target_name):\n",
    "    print(f\"\\nTraining models for {target_name}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Split and scale data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in resource_models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        \n",
    "        if name == 'TensorFlow':\n",
    "            # Create and train TensorFlow model\n",
    "            tf_model = create_tf_regression_model(X_train.shape[1])\n",
    "            \n",
    "            early_stopping = callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=10,\n",
    "                restore_best_weights=True\n",
    "            )\n",
    "            \n",
    "            history = tf_model.fit(\n",
    "                X_train_scaled, y_train,\n",
    "                epochs=100,\n",
    "                batch_size=32,\n",
    "                validation_split=0.2,\n",
    "                callbacks=[early_stopping],\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            y_pred = tf_model.predict(X_test_scaled).flatten()\n",
    "            results[name] = {\n",
    "                'model': tf_model,\n",
    "                'history': history.history,\n",
    "                'predictions': y_pred\n",
    "            }\n",
    "            \n",
    "        else:\n",
    "            # Train traditional models\n",
    "            if name in param_grid_resource:\n",
    "                grid = GridSearchCV(\n",
    "                    model,\n",
    "                    param_grid_resource[name],\n",
    "                    cv=5,\n",
    "                    scoring='neg_mean_squared_error',\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "                grid.fit(X_train_scaled, y_train)\n",
    "                best_model = grid.best_estimator_\n",
    "                print(f\"Best parameters: {grid.best_params_}\")\n",
    "                y_pred = best_model.predict(X_test_scaled)\n",
    "                results[name] = {\n",
    "                    'model': best_model,\n",
    "                    'predictions': y_pred\n",
    "                }\n",
    "            else:\n",
    "                model.fit(X_train_scaled, y_train)\n",
    "                y_pred = model.predict(X_test_scaled)\n",
    "                results[name] = {\n",
    "                    'model': model,\n",
    "                    'predictions': y_pred\n",
    "                }\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        results[name].update({\n",
    "            'mse': mse,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2,\n",
    "            'y_test': y_test\n",
    "        })\n",
    "        \n",
    "        print(f\"MSE: {mse:.4f}\")\n",
    "        print(f\"RMSE: {rmse:.4f}\")\n",
    "        print(f\"R2 Score: {r2:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Train models for each resource target\n",
    "resource_targets = {\n",
    "    'length_of_stay': df['time_in_hospital'],\n",
    "    'num_procedures': df['num_procedures'],\n",
    "    'num_medications': df['num_medications']\n",
    "}\n",
    "\n",
    "resource_results = {}\n",
    "for target_name, y in resource_targets.items():\n",
    "    resource_results[target_name] = train_resource_models(X, y, target_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Readmission Prevention Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_readmission_models(X, y):\n",
    "    print(\"\\nTraining Readmission Prevention Models\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Split and scale data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in readmission_models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        \n",
    "        if name == 'TensorFlow':\n",
    "            # Create and train TensorFlow model\n",
    "            tf_model = create_tf_classification_model(X_train.shape[1])\n",
    "            \n",
    "            early_stopping = callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=10,\n",
    "                restore_best_weights=True\n",
    "            )\n",
    "            \n",
    "            history = tf_model.fit(\n",
    "                X_train_scaled, y_train,\n",
    "                epochs=100,\n",
    "                batch_size=32,\n",
    "                validation_split=0.2,\n",
    "                callbacks=[early_stopping],\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            y_pred_proba = tf_model.predict(X_test_scaled).flatten()\n",
    "            y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "            \n",
    "            results[name] = {\n",
    "                'model': tf_model,\n",
    "                'history': history.history,\n",
    "                'predictions': y_pred,\n",
    "                'pred_proba': y_pred_proba\n",
    "            }\n",
    "            \n",
    "        else:\n",
    "            # Train traditional models\n",
    "            if name in param_grid_readmission:\n",
    "                grid = GridSearchCV(\n",
    "                    model,\n",
    "                    param_grid_readmission[name],\n",
    "                    cv=5,\n",
    "                    scoring='roc_auc',\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "                grid.fit(X_train_scaled, y_train)\n",
    "                best_model = grid.best_estimator_\n",
    "                print(f\"Best parameters: {grid.best_params_}\")\n",
    "            else:\n",
    "                best_model = model\n",
    "                best_model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            y_pred = best_model.predict(X_test_scaled)\n",
    "            y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "            \n",
    "            results[name] = {\n",
    "                'model': best_model,\n",
    "                'predictions': y_pred,\n",
    "                'pred_proba': y_pred_proba\n",
    "            }\n",
    "        \n",
    "        # Calculate metrics\n",
    "        results[name].update({\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_pred_proba),\n",
    "            'y_test': y_test\n",
    "        })\n",
    "        \n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print(f\"ROC-AUC Score: {results[name]['roc_auc']:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Train readmission models\n",
    "readmission_results = train_readmission_models(X, y_readmission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization for Resource Optimization Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set style\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Plot results for each resource target\n",
    "for target_name, results in resource_results.items():\n",
    "    # Create figure for metrics\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot MSE\n",
    "    plt.subplot(1, 3, 1)\n",
    "    mse_scores = [res['mse'] for res in results.values()]\n",
    "    bars = plt.bar(results.keys(), mse_scores)\n",
    "    plt.title(f'MSE by Model ({target_name})')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    for bar in bars:\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n",
    "                f'{bar.get_height():.2f}',\n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    # Plot RMSE\n",
    "    plt.subplot(1, 3, 2)\n",
    "    rmse_scores = [res['rmse'] for res in results.values()]\n",
    "    bars = plt.bar(results.keys(), rmse_scores)\n",
    "    plt.title(f'RMSE by Model ({target_name})')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    for bar in bars:\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n",
    "                f'{bar.get_height():.2f}',\n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    # Plot R2\n",
    "    plt.subplot(1, 3, 3)\n",
    "    r2_scores = [res['r2'] for res in results.values()]\n",
    "    bars = plt.bar(results.keys(), r2_scores)\n",
    "    plt.title(f'R² Score by Model ({target_name})')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    for bar in bars:\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n",
    "                f'{bar.get_height():.2f}',\n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization for Readmission Prevention Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance metrics\n",
    "plt.figure(figsize=(15, 5))\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    scores = [results[metric] for results in readmission_results.values()]\n",
    "    bars = plt.bar(readmission_results.keys(), scores)\n",
    "    plt.title(f'{metric.upper()}')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    for bar in bars:\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n",
    "                f'{bar.get_height():.2f}',\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "for name, results in readmission_results.items():\n",
    "    fpr, tpr, _ = roc_curve(results['y_test'], results['pred_proba'])\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {results[\"roc_auc\"]:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves Comparison')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Summary and Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Performance Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Resource Optimization Summary\n",
    "print(\"\\nResource Optimization Results:\")\n",
    "print(\"-\" * 30)\n",
    "for target_name, results in resource_results.items():\n",
    "    print(f\"\\n{target_name}:\")\n",
    "    best_mse_model = min(results.items(), key=lambda x: x[1]['mse'])[0]\n",
    "    best_r2_model = max(results.items(), key=lambda x: x[1]['r2'])[0]\n",
    "    \n",
    "    print(f\"Best model by MSE: {best_mse_model} (MSE: {results[best_mse_model]['mse']:.4f})\")\n",
    "    print(f\"Best model by R²: {best_r2_model} (R²: {results[best_r2_model]['r2']:.4f})\")\n",
    "\n",
    "# Readmission Prevention Summary\n",
    "print(\"\\nReadmission Prevention Results:\")\n",
    "print(\"-\" * 30)\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "for metric in metrics:\n",
    "    best_model = max(readmission_results.items(), key=lambda x: x[1][metric])[0]\n",
    "    print(f\"Best model by {metric}: {best_model} ({metric}: {readmission_results[best_model][metric]:.4f})\")\n",
    "\n",
    "# Overall Recommendations\n",
    "print(\"\\nRecommendations:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Resource Optimization Recommendations\n",
    "for target_name, results in resource_results.items():\n",
    "    print(f\"\\nFor {target_name}:\")\n",
    "    best_overall = max(results.items(), key=lambda x: x[1]['r2'])[0]\n",
    "    print(f\"Recommended model: {best_overall}\")\n",
    "    print(f\"R² Score: {results[best_overall]['r2']:.4f}\")\n",
    "    print(f\"RMSE: {results[best_overall]['rmse']:.4f}\")\n",
    "\n",
    "# Readmission Prevention Recommendation\n",
    "best_readmission = max(readmission_results.items(), \n",
    "                      key=lambda x: (x[1]['roc_auc'] + x[1]['f1']) / 2)[0]\n",
    "print(\"\\nFor Readmission Prevention:\")\n",
    "print(f\"Recommended model: {best_readmission}\")\n",
    "print(f\"ROC-AUC: {readmission_results[best_readmission]['roc_auc']:.4f}\")\n",
    "print(f\"F1 Score: {readmission_results[best_readmission]['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### readmission models  hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "focused_param_grid = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': randint(100, 1000),\n",
    "        'max_depth': [None] + list(range(10, 110, 10)),\n",
    "        'min_samples_split': randint(2, 20),\n",
    "        'min_samples_leaf': randint(1, 10),\n",
    "        'max_features': ['sqrt', 'log2', None],\n",
    "        'class_weight': ['balanced', 'balanced_subsample', None]\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'num_leaves': randint(20, 100),\n",
    "        'learning_rate': uniform(0.01, 0.2),\n",
    "        'n_estimators': randint(100, 1000),\n",
    "        'min_child_samples': randint(10, 50),\n",
    "        'subsample': uniform(0.6, 0.4),\n",
    "        'colsample_bytree': uniform(0.6, 0.4),\n",
    "        'reg_alpha': uniform(0, 2),\n",
    "        'reg_lambda': uniform(0, 2)\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': randint(100, 1000),\n",
    "        'learning_rate': uniform(0.01, 0.2),\n",
    "        'max_depth': randint(3, 12),\n",
    "        'min_child_weight': randint(1, 10),\n",
    "        'subsample': uniform(0.6, 0.4),\n",
    "        'colsample_bytree': uniform(0.6, 0.4),\n",
    "        'gamma': uniform(0, 2),\n",
    "        'reg_alpha': uniform(0, 2),\n",
    "        'reg_lambda': uniform(0, 2)\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_models_simple(X, y, models_to_tune=['LightGBM', 'Random Forest', 'XGBoost']):\n",
    "    \"\"\"\n",
    "    Simplified model tuning using RandomizedSearchCV\n",
    "    \"\"\"\n",
    "    # Split and scale data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for model_name in models_to_tune:\n",
    "        print(f\"\\nTuning {model_name}...\")\n",
    "        \n",
    "        # Get base model\n",
    "        base_model = readmission_models[model_name]\n",
    "        \n",
    "        # Perform RandomizedSearchCV\n",
    "        random_search = RandomizedSearchCV(\n",
    "            base_model,\n",
    "            focused_param_grid[model_name],\n",
    "            n_iter=50,\n",
    "            scoring='roc_auc',\n",
    "            cv=5,\n",
    "            n_jobs=-1,\n",
    "            random_state=42,\n",
    "            verbose=2\n",
    "        )\n",
    "        \n",
    "        # Fit the model\n",
    "        random_search.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Get best model\n",
    "        best_model = random_search.best_estimator_\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = best_model.predict(X_test_scaled)\n",
    "        y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "        \n",
    "        # Store results\n",
    "        results[model_name] = {\n",
    "            'model': best_model,\n",
    "            'best_params': random_search.best_params_,\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_pred_proba),\n",
    "            'predictions': y_pred,\n",
    "            'pred_proba': y_pred_proba,\n",
    "            'y_test': y_test\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nBest parameters for {model_name}:\")\n",
    "        print(random_search.best_params_)\n",
    "        print(f\"\\nResults for tuned {model_name}:\")\n",
    "        print(f\"ROC-AUC: {results[model_name]['roc_auc']:.4f}\")\n",
    "        print(f\"Accuracy: {results[model_name]['accuracy']:.4f}\")\n",
    "        print(f\"F1 Score: {results[model_name]['f1']:.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize results\n",
    "def plot_tuning_comparison(original_results, tuned_results):\n",
    "    \"\"\"Plot comparison of original and tuned models\"\"\"\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "    models = list(tuned_results.keys())\n",
    "    \n",
    "    fig, axes = plt.subplots(len(metrics), 1, figsize=(10, 4*len(metrics)))\n",
    "    width = 0.35\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        original_scores = [original_results[model][metric] for model in models]\n",
    "        tuned_scores = [tuned_results[model][metric] for model in models]\n",
    "        \n",
    "        x = np.arange(len(models))\n",
    "        axes[i].bar(x - width/2, original_scores, width, label='Original', color='lightblue')\n",
    "        axes[i].bar(x + width/2, tuned_scores, width, label='Tuned', color='lightgreen')\n",
    "        \n",
    "        axes[i].set_title(f'{metric.upper()} Comparison')\n",
    "        axes[i].set_xticks(x)\n",
    "        axes[i].set_xticklabels(models, rotation=45)\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for j, v in enumerate(original_scores):\n",
    "            axes[i].text(j - width/2, v, f'{v:.3f}', ha='center', va='bottom')\n",
    "        for j, v in enumerate(tuned_scores):\n",
    "            axes[i].text(j + width/2, v, f'{v:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "original_results = {k: v for k, v in readmission_results.items() \n",
    "                   if k in ['LightGBM', 'Random Forest', 'XGBoost']}\n",
    "\n",
    "# Tuning process\n",
    "tuned_results = tune_models_simple(X, y_readmission)\n",
    "\n",
    "# Visualize the improvements\n",
    "plot_tuning_comparison(original_results, tuned_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
